\chapter{Implementation}
\label{ch:implementation}

\section{Embedded Refactoring - From Hydra to NanoHydra}\label{sec:im_nanohydra}

    Despite the already good efficiency of Hydra as is, we still identified several potential opportunities for optimization. Some of these optimizations are
    made purely on the algorithmic side, which focus on simplifying superfluous actions, steps and methods that do not penalize accuracy - they are covered in Section \ref{sec:im_nanohydra_algopt}.
    While others relate to the model quantization, which despite being done in preparation for an efficient embedded deployment of the model, can benefit the deployment of the model in any platform - they are covered in Section \ref{sec:im_nanohydra_qnt}, and the subsequent subsections.
    To the new formulation of the model, encompassing all the optimizations and quantizations, we call \textbf{NanoHydra}, to differentiate from the original Hydra model. Only the optimizations are highlighted, and anything else

    \subsection{Algorithm Optimizations}\label{sec:im_nanohydra_algopt}

        \subsubsection{Stage 1: Random Convolutional Kernel Transform}\label{sec:im_nanohydra_algopt_s1}
        
        Like in Rocket \cite{Dempster2020}, Hydra uses kernel weights sampled from the standard normal distribution ($\mathcal{N}(0,1)$) so they can take any real value from that distribution.
        However, the successor algorithm MiniRocket \cite{Dempster2021MR} has shown that RCK-based algorithms can use just integer values sampled from the  set $\{-2,1\}$ without accuracy loss. 
        In fact, it might not even make sense to strictly talk about ``accuracy loss'', since the RCK stage is \emph{quasi}-deterministic, in the sense that the kernel values simply transform the data to be trained later by the classifier,
        which already tries to accommodate the transformed features, by whichever method they were transformed, provided that the kernel can at least crudely approximate the patterns of the input data series (see Section \ref{sec:ha_rck}).
        Furthermore, the specific choice of the set of weights $\{-2,1\}$ in \cite{Dempster2021MR} seemed rather arbitrary and unsubstantiated in experimental results, as the authors simply \emph{recommend} using those values in a way that 
        the values of each kernel sum to zero (i.e. setting 3 weights as -2 and 6 weights as 1, but permuting their locations) in order to subtract any offset the input data might have.

        Therefore, in NanoHydra, we have chosen then to use kernel weights uniformly samples from $\{-1,1\}$, instead of using real numbers sampled from $\mathcal{N}(0,1)$. The benefits of this optimization are twofold

        \begin{itemize}
            \item Reduced memory/flash requirements to store the kernel weights, since \verb|uint8| are enough.
            \item In section \ref{sec:im_nanohydra_qnt_s1}, when we discuss the input quantization, this allows us to use integer multiplications instead of floating-point multiplications.
        \end{itemize}

        \subsubsection{Stage 2: Sparse Scaling}\label{sec:im_nanohydra_algopt_s2}

        In the sparse scaling layer, two optimizations are performed

        \begin{itemize}
        \item \textbf{Omission of the Elementwise Square Root operation} --
        Before the feature vector is used, the original model applies a \textbf{square root} to its elements. 
        The authors in \cite{Dempster2023Hydra} offer no justification to that operation, but one can imagine it is done as a way of reducing the magnitude of the feature values: 
        like in dictionary methods, these result from the accumulation of statistics over the entire length of the input data. 
        However, performing a square root operation carries two inconveniences: it is a relatively costly operation, and it would require the feature vector 
        to use \verb|floating-point| storage so that the square-root operation result remained consistent with the operations done during the training stage.
        We therefore choose to \textbf{omit this operation}, and transform it into an \textbf{arithmetic shift}, which is much less costly and achieves the same purpose.
        In fact, this operation is actually performed in Stage 1 while the values are accumulated in the feature vector, and is the consequence of the quantization
        of this vector which will be further elaborated in Section \ref{sec:im_nanohydra_qnt_s1}

        \item \textbf{Normalization approximated by an arithmetic shift} --
        The purpose of the per-feature normalization is to transform back to the same scale features that can be in 
        radically different scales (e.g. soft-counting accumulates convolution output values, while hard-counting simply accumulates only ones). 
        Therefore, in order for the Logistic Regression classifier to be able to be \textbf{trained effectively}, these features need to be \textbf{normalized to the same scale}; 
        this is in fact the purpose of the normalization, and as such, it does not matter so much that the normalization is mathematically accurate, as long as the features are \emph{approximately} in the same scale. 
        Furthermore, the training of the classifier is performed \textbf{after} this scaling, so it can learn any normalization schemes.

        In the normalization operations, the costlier operation is the division by the standard deviation. We can avoid it by approximating the standard deviation by the closest
        power of two (i.e. instead of using $\sigma$ we use $\sigma\sb{approx} = 2\sp{\text{floor}(\text{log}\sb{2}(\sigma))}$), so that we can replace this division by an arithmetic 
        shift, $x / 2\sp{\sigma\sb{approx}} = x >> \sigma\sb{approx}$, when $x$ is positive - when $x$ is negative, arithmetic shifts are not equivalent to divisions by powers of two because they do not round to zero, 
        so we have to perform the full division, \textbf{but we have replaced a floating-point division with an integer one}!

        \end{itemize}

        \subsubsection{Stage 3: Classifier}\label{sec:im_nanohydra_algopt_s3}
        No specific algorithmic optimization was introduced in this stage. In Section \ref{sec:im_nanohydra_qnt_s3}, the quantization scheme of this stage is discussed.

    \subsection{Quantization}\label{sec:im_nanohydra_qnt}

        \subsubsection{Stage 1: Random Convolutional Kernel Transform}\label{sec:im_nanohydra_qnt_s1}

        As discussed in Section \ref{sec:im_nanohydra_algopt_s1}, the kernel weights are fixed to a set of two values, $\{-1,1\}$. Although the purpose was 
        algorithmic simplification and the possibility of performing operations with fixed-point integers, it can also be seen as a quantization step. 
        However, it is a quantization step done \textbf{before} the training process - recall that the RCK transform the inputs 
        into a feature vector, and these feature vectors are then collectively used as the training set - and so the training \textbf{already takes into account} the quantization; this
        stands in stark contrast with other methods (e.g. Recursive Neural Networks, Deep Learning Methods, CNNs) where weight quantization is an undesirable step performed \textbf{after} the training process has concluded, and 
        done on the \emph{trained} full-precision floating point training parameters, with the sole purpose of compressing the model size and improve the efficiency of the computations.
        Besides simplifying the training process (only the last layer, the classifier in Stage 3, is quantized \emph{post-training}), it allows us to perform the most computationally intensive and
        information extraction critical step in the most efficient way possible.

        Of course, for this to be possible, the input itself needs to be quantized. We have chosen a 16-bit fixed-point quantization scheme, where the inputs are automatically quantized to
        $Qn.m$ format, by using the training set to evaluate the maximum span of values, which defines the number of integer bits $n$
        that are needed to represent all values without overflow. It then follows that the number of fractional bits $m$ are calculated knowing that $B=n+m$, where $B$ is the number of bits 
        of the representation type chosen - in this case $B=16$. The rationale for this choice, over choosing 32 bits for instance, is that we can perform the convolution operations using 
        2-way SIMD operations, while still maintaining a good level of fidelity in the input signal.

        Recalling Equation \ref{eq:dil_cnn_eq}, we see that the convolution operations are nothing more that a series of 9 multiply accumulate operations, 
        where both operands are 16-bit signed integers ($\{-1,1\}$ are cast to 16-bit signed integers, to make the computation more efficient). Therefore, the partial calculations
        must be accumulated in a 32-bit temporary variable to prevent overflows in the intermediate calculations. Furthermore, noting the remarks from \ref{sec:ha_s1}, after the convolutional
        output has been performed for a given time step, and its value has been accumulated in the feature vector, we have no further use for it. The main benefit from this fact is that we do not 
        need to store the convolutional outputs for all time-steps, making the memory requirements \textbf{are independent of the input data series length} much more scalable in relation to the input length.

        However, we should note lastly that since the convolutional outputs $y[n]$ are intermediately stored in 32-bit, it can happen that its final value indeed needs more than 16-bits to be stored
        (e.g. suppose all kernel weights are one, and the input data points are all equal to the maximum positive value of a signed 16-bit integer, the result needs more than 16-bits to be correctly represented).
        This would require our feature vector to use 32-bit signed representation, which double its memory requirements, compared to using a signed 16-bit representation like the convolution operands. To keep the latter
        representation, we proposed performing an \textbf{an arithmetic shift} on the convolution output \textbf{before} its accumulation in the feature vector, such that the cast from 32-bit to 16-bit and that subsequent accumulations at that index do not overflow.
        \footnote{This issue indeed happened during the development, and was one of the issues that consumed some debugging effort, since the Python Model vs C Implementation equivalence check was failing. 
        The arithmetic shift solution was formulated precisely to tackle this issue}
        Note that this is done at the expense of discarding some fractional bits, which have proven to cause no accuracy degradation. This is exactly one of the benefits of using a dictionary method for crafting these features: the
        long term statistics of the occurrence of a given pattern dominate over small local variations, making it more resilient to noise. It is in itself a form of implicit \textbf{regularization}.

        % As discussed before, weights are now integer, which is a form of quantization
        % We need to quantize the input
        % Choose 16 bits, as this way we can use two-way SIMD
        % Time samples independent of each other, calculated one by one, convolutional output of current time sample stored as 32 bit to avoid overflow
        % Accumulatiors have shifts, so that summations does not overflow 16 bits - otherwise, feature vector would double in size (must have been 32 bit)

        \subsubsection{Stage 2: Sparse Scaling}\label{sec:im_nanohydra_qnt_s2}

        In the sparse scaling step, according to what was already explained in Section \ref{sec:ha_algo_descr}, the normalization requires us to store two trainable vectors: the means $\mu$ and the standard deviations $\sigma\sb{approx}$.
       
        Regarding the means, we should recall that our feature vector that results from Step 1 of the algorithm is already quantized into a signed 16-bit integer.
        Therefore, after all the input samples are transformed after Step 1, the training step simply calculates the per-feature average, clips and casts the resulting
        values to 16-bit signed integers. We note that the averaging operation conveniently leaves unchanged the place of the point, so no further action is needed: the resulting 
        values will be zero-centered in whichever fixed-point representation they were before.
        
        Regarding the standard deviations, as defined in Section \label{sec:im_nanohydra_algopt_s2}, we approximate by the closest power of two, but we only need to store the power, therefore each element
        is stored as an 8-bit signed integer.
        % Means quantized to 16 bit
        % Arithmetic shift is a form of quantization, std quantized fo 8 bit

        \subsubsection{Stage 3: Classifier}\label{sec:im_nanohydra_qnt_s3}

        The classifier, being a simple matrix-vector multiplication as defined in Section \ref{sec:ha_s3} (vector is the sparse-scaled feature vector, the matrix is the fully-connected layer), can be easily
        be quantized in 8-bit without overflows and no loss of classification accuracy. This is contingent on the assumption that the Logistic Regression classifier that originated the weights was properly 
        regularized -- which prevents the weights from ever-growing exceedingly large -- and that the feature vector was scaled appropriately, as it is the case - which prevents the weights from ever \emph{needing}
        to grow exceedingly large. The accumulation variable is chosen to be a signed 32-bit integer for two reasons:
        
        \begin{itemize}
            \item Since we only need one of such variables per output class, and this value is generally, at most, on the order of tens, the memory footprint is still rather small. 
            \item The number of intermediate MAC operations is the same as the length of the feature vector, which can be up to a thousand elements long, increasing the probability of intermediate calculation
                  overflows. Therefore, for such a small memory footprint, it is logical to offset that risk.
        \end{itemize}

        An 8-bit representation is desired, as this allows for 4-way SIMD computations.

\section{Code Implementation}\label{sec:im_nanohydra_codeimp}

    \subsection{General Strategy and Structure}\label{sec:im_nanohydra_genstrat}

    \subsubsection{Python model}
    The starting point of this work was the python model provided by the Hydra authors. From there, we implemented the algorithmic changes described in Section \ref{sec:im_nanohydra_algopt}
    which transformed it into the proposed NanoHydra model, and that refactored python model was trained against the entire UCR Dataset Archive. This served as a way to evaluate the correctness of those changes 
    and the maintenance of an adequate accuracy level across the entire archive, which was done via a python script infrastructure developed from scratch
    that automates that procedure. The Python model operates in \textbf{batches} of input samples.

    \subsubsection{Speeding up the Python model with Cython}
    From the point of view of performance, the critical step is Step 1, the RCK transform. Although this algorithm already trains much faster than other state-of-the-art methods \cite{Dempster2023Hydra},
    we can further speed-up the training and inference by using Cython\footnote{\href{https://cython.org/}{https://cython.org/}}. This library allows the programmer to write certain performance-critical functions
    in a python dialect that can be translated to C, and compiled to machine code before run-time. Depending on the skill of the programmer it can completely bypass the Python Interpreter and be run directly in machine code.
    This is especially useful since the Python Interpreter Lock (PIL) prevents multicore execution of Python code (one needs to resort to the \verb|multiprocessing|\footnote{\href{https://docs.python.org/3/library/multiprocessing.html}{https://docs.python.org/3/library/multiprocessing.html}} library, and use process
    based parallelism), but by using Cython we can add \textbf{OpenMP}\footnote{\href{https://www.openmp.org/}{https://www.openmp.org/}} \textbf{directives} that are appended to the sections of the Cython function that exclusively use C, and therefore are outside the PIL. This results
    in easy multicore parallelization gains, as we would have when using C+OpenMP, while still having the expressiveness and productivity of a Python environment. Lastly, it should be noted that the rationale for using Cython when the original
    formulation used torch is that of greater flexibility of trying new ideas in a language that closely resembled the target embedded deployment, which would be in C.

    \subsubsection{C implementation}\label{sec:im_cim}
    The algorithm implemented in the aforementioned Python model was then ported to C, with the end goal of deploying it in an embedded system. 
    However, since we must guarantee that the C implementation is equivalent to our Python model, the C implementation is first done in a platform-agnostic way,
    such that it can be run on a PC, and quickly run across an entire dataset, check its output is equivalent bit-by-bit to the output of the Python model. With this approach, 
    it is much easier to debug and has greatly sped up the development and debug cycle (e.g. the \textbf{entire} ECG5000 dataset can be inferred in 0.5 seconds, versus 10 minutes in the GAP9 simulator).
    
    Furthermore, the current C code is highly portable and configurable, and we can switch between the operation modes at compile time by toggling \textbf{compilation switches}.
    The existing operations modes are summarized in Table \ref{tbl:compile_switches}

    \begin{table}[h!]
    \begin{tabular}{||p{1.5in}|p{4in}||}
        \hline
        Compilation Switch & Function  \\

        \hline\hline
        \verb|TARGET_GAP9| & When defined, enables GAP9-specific code sections. When not defined, code compiles in agnostic mode \\
        \hline
        \verb|PARALLELIZE| & When defined, enables parallelization of the RCK computations. \\
        \hline
        \verb|VECTORIZE|   & When defined, enables vectorization across all computation stages of the algorithm. \\
        \hline
        \verb|QUANT_8BIT|  & When defined, uses 4-way SIMD vectorization (8-bit operands) on the RCK computations. When not, SIMD defaults to 2-way (16-bit operands). \\
        \hline
    \end{tabular}
    \caption{Compilation Switches}
    \label{tbl:compile_switches}
    \end{table}

    The C implementation of the NanoHydra code is distributed in a library. Its constituent functions, and their description, is
    provided in Table \ref{tbl:library_c}

    \begin{table}[h!]
    \begin{tabular}{||p{1.5in}|p{4in}||}
        \hline
        Function & Description  \\

        \hline\hline
        \verb|hydra_init| & Initializes the state variables, and allocates the necessary memory. \\
        \hline
        \verb|hydra_reset| & Resets the state vectors, should be called after one input sample has been processed, and before starting the processing of the next one. \\
        \hline
        \verb|hydra_convolve|   & Computes the RCK over all kernel groups, for a given $N\sb{\text{diff}}$ and dilation $d$. Internal function, meant to be called inside \verb|hydra_forward|\\
        \hline
        \verb|hydra_forward|   & Represents a complete computation of the Stage 1 of the algorithm, computing the RCK over all kernel groups, and for all $N\sb{\text{diff}}$ and $D$\\
        \hline
        \verb|hydra_sparse_scale|   & Represents a complete computation of Stage 2 of the algorithm, should be called after \verb|hydra_forward|\\
        \hline
        \verb|hydra_classifier|     & Represents a complete computation of Stage 3 of the algorithm, should be called after \verb|hydra_classifier|. Outputs the Logistic Regression scores, which are then used to generate a prediction (e.g. \verb|argmax|).\\
        \hline
    \end{tabular}
    \caption{NanoHydra Library}
    \label{tbl:library_c}
    \end{table}

    \section{Embedded Implementation}\label{sec:im_nanohydra_embimp}

        In terms of the GAP9 port of the C model of NanoHydra outlined in Section \ref{sec:im_cim}, some aspects are notable of mention. The general methodology was to first perform
        as many general optimizations as possible on the C port, before performing GAP9-specific optimizations, since these general optimizations will also optimize the GAP9 port, but
        are easier to debug and validate, since they are run on the development PC. However, the structure of the C port, although initially generic, was thought out with the future GAP9 Architecture outlined in Section \ref{sec:im_nanohydra_embimp_genopt} in mind.
        In that section, we will explain the generic optimizations as well.

        The GAP9-specific optimizations were the use of \textbf{Vectorization} and \textbf{Parallelization}, whose implementation will be explained in Sections \ref{sec:im_nanohydra_embimp_vect} and \ref{sec:im_nanohydra_embimp_paral}, respectively.

        \subsection{General Architecture}\label{sec:im_nanohydra_embimp_genopt}

        \subsubsection{C Optimizations}
        As referenced before, Step 1 of algorithm produces a feature vector, which is a result of the concatenation of the RCK passes through dilated and differentiated versions of the input time series. It was chosen to linearize
        the feature vector instead of using a multidimensional array for each group, dilation and differentiation level, since this would require several levels of pointer dereferencing. Furthermore, since the features are concatenated in 
        a consistent fashion, we can slice the portion of the feature vector that each group of kernels is working on, get a pointer to each of those locations, and avoid repeating the pointer arithmetic computations on every access. This resulted
        in performance gains of up to 20\%.

        \subsubsection{Usage of the GAP9 Compute Cluster}\label{sec:im_nanohydra_usage_cc}
        Using the GAP9 CC comes with great potential execution time gains, due to the possibility of parallelizing a work load over 8 cores. However, the CC should be switched off after the parallel computation finishes, since
        its chip domain consumes extra power; and whenever it is powered down, the L1 RAM contents are lost, since they are volatile. This means that for every power cycle there is considerable memory traffic from the shared L2 RAM into
        the L1 RAM of the CC (on which CC has lower access latency), which is coordinated and streamlined by the DMA block.

        Therefore, the decision of using the CC is a balance between:        
        \begin{itemize}
            \item Execution time gains one gets when using it.
            \item Additional memory traffic overhead of operands, parameters and results (Copy-In for operands and parameters, Copy-Out for results) at the DMA.
            \item Additional power required by using the CC, versus keeping computations in the FC.
        \end{itemize}
        Execution time gains on the CC can be offset by DMA traffic, if the parallelizable section is too short, but the volume of operands is very high. Furthermore, 
        blind execution time optimization can degrade energy-efficiency -- if upper bounds requirements on the execution time are defined, then we only need to use the CC if the computation time is too long and violates these requirements; if it does not, 
        further optimizing execution time makes no sense, since we increase power consumption on a design that already satisfies requirements.

        With these considerations in mind, we can then justify if the usage of the CC is desirable for each of the three computation stages of NanoHydra. Using the
        memory and computational requirements listed in Tables \ref{tbl:gap9_test_cfg_reqmem} and \ref{tbl:gap9_test_cfg_reqmem}, respectively, we can generate
        another table, Table \ref{tbl:im_nanohydra_rel_eff}, with the relative memory and computational effort over the total forward inference pass.

        \begin{table}[h!]
            \centerfloat
            \rowcolors{2}{lightgray}{white}
            \begin{tabular}{ c c c c c}
            \toprule
            \textbf{Configuration Name} & Computation & Memory & Computation & Memory \\
            \midrule
            CFGA & 99.78 \% &  1.83 \%  &  0.22 \% & 98.17 \% \\
            CFGB & 99.78 \% &  0.56 \%  &  0.22 \% & 99.44 \% \\
            \bottomrule
            \end{tabular}
            \caption{Computational and memory relative effort comparison.}%
            \label{tbl:im_nanohydra_rel_eff}
        \end{table}

        An interesting 99\% vs. 1\% pattern emerges: Step 1 does 99\% of the computational effort, but requires only 1\% of the total memory requirements, while Step 2 displays the complete opposite characteristics! We see then
        that parallelization greatly benefits Step 1, since most of the computational effort is done at this stage, and to make matters even better, the memory traffic is only around 1 \% of the total, which makes any DMA
        traffic overhead insignificant. However, Steps 2 and 3 only represent 1 \% of the computational effort, but the memory traffic that would be generated would be 100x greater than Step 1. Therefore, not only the parallelization
        gains would benefit only 1 \% of the computational effort, we would incur in a large amount of DMA overhead which would effectively cancel any parallelization computation speed up (given the extreme imbalance, it would most certainly worsen it).

        The reason for this imbalance is that in Step 1, the RCKs are intensive and repetitive operations done on dilated and differentiated versions of the input time series with a small set of fixed parameters, while Steps 2 and 3 are dominated by 
        matrix-vector products, where the matrices are the trained coefficients of the Logistic Regression classifier.

        \subsection{Vectorization}\label{sec:im_nanohydra_embimp_vect}

        Regardless of the relative computational effort, vectorization can always improve the execution speed. 
        The actual benefit depends more on the types of computations that each stage performs, and if the operands are already packed in memory (if they are not, we need to pack them before executing the SIMD instruction).

        Vectorization is accomplished by making use of the GAP9 built-in intrinsics that map to SIMD instructions. The instructions used were:

        \begin{itemize}
            \item \textbf{Packing}     -- \verb|__builtin_pulp_pack2| and \verb|__builtin_pulp_pack4| for packing 16-bit and 8-bit operands, respectively, that are not aligned in memory.
            \item \textbf{Dot Product} -- \verb|__builtin_pulp_sdotsp2| and \verb|__builtin_pulp_sdotsp4| for performing 2-way and 4-way SIMD, using 16-bit or 8-bit operands, respectively. 
                                          The accumulation scalar variable is one of the arguments of the functions, and the updated value is returned.
        \end{itemize}

        Regarding Step 1, one of the main operation is the convolution, portrayed in Equation \ref{eq:dil_cnn_eq}, which can be seen as a \emph{dot product} between the kernel
        weights, and a window of the input time series (whose samples can have dilation or not). This dot product, whose length is fixed at 9 samples, can be either vectorized as 
        \textbf{two} 4-way SIMD steps, if the input is quantized in 8-bit, using \verb|__builtin_pulp_sdotsp4|, or as \textbf{four} 2-way SIMD steps if the input is quantized in 8-bit, 
        using \verb|__builtin_pulp_sdotsp2|: in both cases, since the number of operations is odd, the ninth operands (the last one) must be accumulated as a scalar operation.
        The kernel weights are already aligned in memory by design, since they are stored as arrays with the correct operand type (\verb|int16| or \verb|int8|), therefore they do not
        need an intermediate packing step, which would add computational overhead: we simply need to get dereference a pointer of the weight array at the correct place, and cast it to the
        vectorized type, \verb|v2s| or \verb|v4s|. The input array, however, since we can perform operations on dilated windows of its input, the dot product operands are not aligned
        for dilation scales greater than one. We then inevitably incur the small penalty of packing them using \verb|__builtin_pulp_pack2| or \verb|__builtin_pulp_pack4|, accordingly.

        In terms of Step 2, experiments have shown during development that vectorizing the computations did not improve the execution time. This is due to the irregularity of the computational
        flow, since the scaling operations are selectively performed on the feature vector depending on the current sample value.

        Lastly, Step 3 is also vectorized, with a fixed 4-way SIMD, since experiments during development revealed no impact in accuracy of quantizing the Logistic Regression weights in 8-bit,
        instead of using 16-bit or event 32-bit. 

        Table \ref{tbl:vect_summary} summarizes the above discussion, enumerating the vectorization levels on each of the algorithm computation steps.

        \begin{table}[H]
        \begin{tabular}{||p{1.5in}|p{1.5in}|p{2.5in}||}
            \hline
            Stage & Vectorized ? & Rationale  \\

            \hline\hline
            Stage 1 - RCK            & 2-way or 4-way SIMD & Operands are 16-bit, or 8-bit, but not always packed (dilations).\\
            \hline
            Stage 2 - Sparse Scaling & No         & Calculation flow is irregular, performance did not improve.\\
            \hline
            Stage 3 - Classifier     & 4-way SIMD & Weights are 8-bit, operand vector is 16-bit, but its values span 8-bit, so they are packed on-the-fly.\\
            \hline
        \end{tabular}
        \caption{Vectorization per computation stage}
        \label{tbl:vect_summary}
        \end{table}

        \subsection{Parallelization}\label{sec:im_nanohydra_embimp_paral}

        Taking into account the discussion and conclusions from Section \ref{sec:im_nanohydra_usage_cc}, we choose to only parallelize the most computationally intensive step
        of the algorithm, Step 1. 

        The choice is made to parallelize the computation at the \textbf{group} level. The reason for this that NanoHydra topologies always use a minimum Of
        \textbf{sixteen} groups of \texbf{eight} kernels each. Therefore, we are guaranteed to always maximize the usage of all eight cores, even in the smallest
        usable topologies of the model. Furthermore, this means parallelizing at the outermost level of the \verb|hydra_convolve| function, maximizing the amount of time
        each core spends in computation, avoiding introducing many synchronization barriers which would add overhead. 

        The CC of GAP9 follows a fork-join computation paradigm similar to that of GPUs. Each core has its own stack, but they run the same instructions and operate on the
        shared L1 memory space. Through the function \verb|pi_core_id()| each core knows its ID, and we can then use this value to share the workload among the cores, by restricting
        each core's activities to a given section of the workload. In our case, we use the group index as the argument to this comparison. This follows a more efficient and
        simple \emph{bare metal} parallelization scheme, without having to resort to OpenMP pragmas.

        \begin{table}[H]
        \begin{tabular}{||p{1.5in}|p{1.5in}|p{2.5in}||}
            \hline
            Stage & Parallelized ? & Rationale  \\

            \hline\hline
            Stage 1 - RCK            & Yes & Embarrassingly parallel section of the computation. Parallelized across the groups, inside \verb|hydra_convolve|.\\
            \hline
            Stage 2 - Sparse Scaling & No  & Computation is much shorter than Stage 1, but the memory traffic to the compute cluster would be significantly higher, offsetting any parallelization gains.\\
            \hline
            Stage 3 - Classifier     & No  & Same justification as for Stage 2.\\
            \hline
        \end{tabular}
        \caption{Parallelization per computation stage}
        \label{tbl:vect_summary}
        \end{table}